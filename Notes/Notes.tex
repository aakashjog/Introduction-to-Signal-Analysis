\documentclass[titlepage, fleqn, a4paper, 12pt, twoside]{article}
\usepackage{etex}
\usepackage{geometry}
\usepackage{exsheets} %question and solution environments
\usepackage{amsmath, amssymb, amsthm} %standard AMS packages
\usepackage[utf8]{inputenc}
\usepackage{esint} %integral signs
\usepackage{marginnote} %marginnotes
\usepackage{gensymb} %miscellaneous symbols
\usepackage{commath} %differential symbols
\usepackage{xcolor} %colours
\usepackage{cancel} %cancelling terms
\usepackage[free-standing-units,space-before-unit]{siunitx} %formatting units
	\sisetup
	{
		per-mode=fraction,
		fraction-function=\frac
	}
\usepackage{tikz, pgfplots} %diagrams
	\usetikzlibrary{calc, hobby, patterns, intersections, angles, quotes, spy}
\usepackage{graphicx} %inserting graphics
\usepackage{hyperref} %hyperlinks
\usepackage{datetime} %date and time
\usepackage{enumerate, enumitem} %numbered lists
\usepackage{float} %inserting floats
\usepackage[american voltages]{circuitikz} %circuit diagrams
\usepackage{setspace} %double spacing
\usepackage{microtype} %micro-typography
\usepackage{listings} %formatting code
	\lstset{language=Matlab}
	\lstdefinestyle{standardMatlab}
	{
		belowcaptionskip=1\baselineskip,
		breaklines=true,
		frame=L,
		xleftmargin=\parindent,
		language=C,
		showstringspaces=false,
		basicstyle=\footnotesize\ttfamily,
		keywordstyle=\bfseries\color{green!40!black},
		commentstyle=\itshape\color{purple!40!black},
		identifierstyle=\color{blue},
		stringstyle=\color{orange},
	}
\usepackage{algpseudocode} %algorithms
\usepackage{algorithm} %algorithms
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{todonotes}
\usepackage{extarrows}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} %adds numbers to specific equations in non-numbered list of equations

\theoremstyle{definition}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}

\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}
\newtheorem{law}{Law}

\makeatletter
\@addtoreset{section}{part} %resets section numbers in new part
\makeatother

\newcommand\blfootnote[1]{%
	\begingroup
	\renewcommand\thefootnote{}\footnote{#1}%
	\addtocounter{footnote}{-1}%
	\endgroup
}

\DeclareMathOperator{\sinc}{sinc}

\newcommand{\FS}{\xleftrightarrow{\mathrm{FS}}}
\newcommand{\DFS}{\xleftrightarrow{\mathrm{DFS}}}

\renewcommand{\marginfont}{\scriptsize \color{blue}}

\renewcommand{\tilde}{\widetilde}

\SetupExSheets{solution/print = true} %prints all solutions by default

%opening
\title{Introduction to Signal Analysis}
\author{Aakash Jog}
\date{2015-16}

\begin{document}

\maketitle
\pagenumbering{roman}
\begin{titlepage}
\newgeometry{margin=0cm}
\maketitle
\end{titlepage}
\restoregeometry
%\setlength{\mathindent}{0pt}

\blfootnote
{	
	\begin{figure}[H]
		\includegraphics[height = 12pt]{cc.pdf}
		\includegraphics[height = 12pt]{by.pdf}
		\includegraphics[height = 12pt]{nc.pdf}
		\includegraphics[height = 12pt]{sa.pdf}
	\end{figure}
	This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. To view a copy of this license, visit \url{http://creativecommons.org/licenses/by-nc-sa/4.0/}.
} %CC-BY-NC-SA license

\tableofcontents

\clearpage
\pagenumbering{roman}

\section{Lecturer Information}

\textbf{Yaniv Isbi}\\
~\\
E-mail: \href{mailto:isbi@eng.tau.ac.il}{isbi@eng.tau.ac.il}\\
Mobile: \href{tel:+972522850915}{+972 52 285 0915}

\section{Instructor Information}

\textbf{Tom Mahler}\\
~\\
E-mail: \href{tommahle@mail.tau.ac.il}{tommahle@mail.tau.ac.il}

\section{Required Reading}

\begin{enumerate}
	\item Alan V. Oppenheim, Alan S. Willsky, S. Hamid Nawab, Signals \& Systems, Prentice Hall, 2nd edition, 1997
	\item Alan V. Oppenheim, Ronald W. Schafer, John R. Buck, Discrete-Time Signal Processing, Prentice Hall, 2nd edition, 1999
\end{enumerate}

\clearpage
\pagenumbering{arabic}

\part{Basics of Signals and Systems}

\section{Periodic Signals}

\begin{definition}[Continuous time signal]
	A signal $x(t)$, where $t \in \mathbb{R}$ is a continuous variable, is called a continuous time signal.
\end{definition}

\begin{definition}[Discrete time signal]
	A signal $x[n]$, where $n \in \mathbb{N}$ is a discrete variable, is called a discrete time signal.
\end{definition}

\begin{definition}[Periodic continuous time signal]
	A signal $x(t)$ is said to be periodic if $\forall t$, $\exists T$, such that
	\begin{align*}
		x(t) & = x(t + T)
	\end{align*}
	The smallest such $T$ is called the fundamental period.
	It is denoted as $T_0$.
\end{definition}

\begin{definition}[Periodic discrete time signal]
	A signal $x[n]$ is said to be periodic if $\forall n$, $\exists N \in \mathbb{N}$, such that
	\begin{align*}
		x[n] & = x[n + N]
	\end{align*}
	The smallest such $N \in \mathbb{N}$ is called the fundamental period.
	It is denoted as $N_0$.
\end{definition}

\begin{question}
	Is
	\begin{align*}
		x(t) & = \cos t
	\end{align*}
	periodic?
\end{question}

\begin{solution}
	\begin{align*}
		\cos(t) & = \cos(t + 2 \pi)
	\end{align*}
	Therefore, the function is periodic, with fundamental period $2 \pi$.
\end{solution}

\begin{question}
	Is
	\begin{align*}
		x[n] & = e^{j \omega_0 n}
	\end{align*}
	periodic?
\end{question}

\begin{solution}
	\begin{align*}
		x[n]                & = e^{j \omega_0 n} \\
		\therefore x[n + N] & = e^{j \omega_0 (n + N)}
	\end{align*}
	Therefore,
	\begin{align*}
		x[n]                        & = x[n + N]                          \\
		\iff e^{j \omega_0 n}       & = e^{j \omega_0 (n + N)}            \\
		\iff e^{j \omega_0 n}       & = e^{j \omega_0 n} e^{j \omega_0 N} \\
		\iff e^{j \omega_0 N}       & = 1                                 \\
		\iff \omega_0 N             & = 2 \pi m                           \\
		\iff \frac{\omega_0}{2 \pi} & = \frac{m}{N}
	\end{align*}
	Therefore, as both $m$ and $N$ are natural numbers, $\frac{m}{N} \in \mathbb{Q}$.
	Therefore, the function is periodic if and only if $\frac{\omega_0}{2 \pi}$ is a rational number.
\end{solution}

\section{Set of Harmonically Related Functions}

\begin{definition}[Fundamental frequency]
	The frequency which corresponds to the fundamental period of a signal is called the fundamental frequency.
\end{definition}

\begin{definition}[Harmonically related functions]
	The set of all functions $\varphi_k(t)$, each with fundamental frequency
	\begin{align*}
		\Omega_k & = k \Omega_0
	\end{align*}
	is called a set of harmonically related functions.\\
	Each $\Omega_k$ is called the $k$th harmonic.
\end{definition}

\begin{theorem}
	The fundamental periods of harmonically related functions are given by
	\begin{align*}
		\tilde{T} & = \frac{T_0}{k}
	\end{align*}
	where $T_0$ is the fundamental period corresponding to the fundamental period $\Omega_0$.
\end{theorem}

\begin{proof}
	Let
	\begin{align*}
		x(t) & = e^{j \Omega t}
	\end{align*}
	Therefore,
	\begin{align*}
		T_0 & = \frac{2 \pi}{\Omega_0}
	\end{align*}
	is the fundamental period of $x(t)$.\\
	Therefore,
	\begin{align*}
		\Omega_0 & = \frac{2 \pi}{T_0}
	\end{align*}
	is the fundamental frequency of $x(t)$.\\
	Therefore,
	\begin{align*}
		\Omega_k & = k \Omega_0 \\
                         & = k \frac{2 \pi}{T_0}
	\end{align*}
	where $k \in \mathbb{Z}$.\\
	Let
	\begin{align*}
		\varphi_k(t) & = e^{j \Omega_k t} \\
                             & = e^{j k \Omega_0 t}
	\end{align*}
	Therefore,
	\begin{align*}
		\varphi_j(t + \tilde{T})        & = e^{j k \Omega_0 (t + \tilde{T})} \\
		\iff e^{j k \Omega_0 \tilde{T}} & = 1                                \\
		\iff \tilde{T}                  & = \frac{T_0}{k}
	\end{align*}
\end{proof}

\subsection{Number of Harmonics in Discrete Time Systems}

Let
\begin{align*}
	x[n] & = e^{j \omega n}
\end{align*}
Let the fundamental period be $N$.\\
Therefore, the fundamental frequency is
\begin{align*}
	\omega_0 & = \frac{2 \pi}{N}
\end{align*}
where $N \in \mathbb{N}$ is the fundamental period of $x[n]$.\\
Therefore,
\begin{align*}
	\varphi_0[n] & = e^{j \omega_0 n} \\
                     & = e^{j \frac{2 \pi}{N} n}
\end{align*}
Therefore,
\begin{align*}
	\varphi_k[n] & = e^{j k \omega_0 n}
\end{align*}
Therefore,
\begin{align*}
	\varphi_{k + N}[n] & = e^{j (k + N) \omega_0 n}                     \\
                           & = e^{j k \omega_0 n} e^{j N \omega_0 n}        \\
                           & = e^{j k \omega_0 n} e^{j N \frac{2 \pi}{N} n} \\
                           & = e^{j k \omega_0 n}                           \\
                           & = \varphi_k[n]
\end{align*}
Therefore, every $N$th frequency $\omega_k$ is equal.
Therefore, there are exactly $N$ distinct frequencies, and hence $N$ corresponding distinct harmonics.

\section{Properties of Systems}

\subsection{Memory}

\begin{definition}[Memoryless systems]
	A system is said to be memoryless if and only if the output at $t$, or at $n$, is dependent only on the input of at the same $t$, or $n$.
\end{definition}

\begin{definition}[Accumulator]
	A system such that
	\begin{align*}
		y[n] & = \sum\limits_{m = -\infty}^{n} x[m]
	\end{align*}
	where $y[n]$ is the output, and $x[n]$ is the input, is called an accumulator.
\end{definition}

\begin{theorem}
	Every system with memory can be formulated in terms of feedback systems.
\end{theorem}

\begin{question}
	Formulate a accumulator such that
	\begin{align*}
		y[n] & = \sum\limits_{m = -\infty}^{n} x[m]
	\end{align*}
	in terms of feedback systems.
\end{question}

\begin{solution}
	\begin{align*}
		y[n] & = \sum\limits_{m = -\infty}^{n} x[m]            \\
                     & = x[n] + \sum\limits_{m = -\infty}^{n - 1} x[m] \\
                     & = x[n] + y[n - 1]
	\end{align*}
\end{solution}

\subsection{Invertibility}

\begin{definition}[Invertible system]
	A system with input $x$ and output $y$ is said to be invertible if and only if there exists a system with input $y$ and output $x$.
\end{definition}

\subsection{Causality}

\begin{definition}[Causal system]
	A system with input $x(t)$ and output $y(t)$ is said to be causal if and only if $y(t)$ is independent of $x(\tau)$, for $\tau > t$.
\end{definition}

\begin{question}
	Is the system
	\begin{align*}
		y[n] & = \frac{1}{2 m + 1} \sum\limits_{k = -m}^{m} x[n + k]
	\end{align*}
	causal?
\end{question}

\begin{solution}
	As $y[n]$ is dependent on $x[n + k]$, with $k > 0$, the system is non-causal.
\end{solution}

\subsection{BIBO Stability}

\begin{definition}[BIBO stable system]
	A system is said to be BIBO stable, if and only if the output for a bounded input is bounded.
\end{definition}

\begin{question}
	Let
	\begin{align*}
		y[n] & = \sum\limits_{k = -\infty}^{n} x[k]
	\end{align*}
	Let
	\begin{align*}
		x[n] &= u[n]\\
		&=
			\begin{cases}
				0 & ;\quad n < 0   \\
				1 & ;\quad 0 \le n \\
			\end{cases}
	\end{align*}
	Show that the system is BIBO unstable.
\end{question}

\begin{solution}
	For all $n$,
	\begin{align*}
		\left| x[n] \right| & \le 1
	\end{align*}
	Therefore, the input is bounded.\\
	However,
	\begin{align*}
		\lim\limits_{n \to \infty} y[n] & = \lim\limits_{n \to \infty} \sum\limits_{k = -\infty}^{n} x[k]
	\end{align*}
	Therefore, the limit is infinite.
	Hence, the system is BIBO unstable.
\end{solution}

\subsection{Time Independence}

\begin{definition}[Time independent/invariant system]
	A system is said to be time independent if and only if the response to a time-shifted input is the response to the input, shifted by the same amount, i.e., if
	\begin{align*}
		y[n] & = H\left\{ x[n] \right\}
	\end{align*}
	then,
	\begin{align*}
		y[n - n_0] & = H\left\{ x[n - n_0] \right\}
	\end{align*}
	Similarly, if
	\begin{align*}
		y(t) & = H\left\{ x(t) \right\}
	\end{align*}
	then,
	\begin{align*}
		y(t - t_0) & = H\left\{ x(t - t_0) \right\}
	\end{align*}
\end{definition}

\begin{question}
	Is the system
	\begin{align*}
		y[n] & = x[n] - x[n - 1]
	\end{align*}
	time invariant?
\end{question}

\begin{solution}
	Let
	\begin{align*}
		\tilde{x}[n] & = x[n - n_0]]
	\end{align*}
	Therefore,
	\begin{align*}
		H\left\{ \tilde{x}[n] \right\} & = \tilde{x}[n] - \tilde{x}[n - 1] \\
                                               & = x[n - n_0] - x[n - n_0 - 1]     \\
                                               & = y[n - n_0]
	\end{align*}
	Therefore, the system is time invariant.
\end{solution}

\begin{question}
	Is the system
	\begin{align*}
		y(t) & = \sin(t) x(t)
	\end{align*}
	time invariant?
\end{question}

\begin{solution}
	Let
	\begin{align*}
		\tilde{x}(t) & = x(t - t_0)
	\end{align*}
	Therefore,
	\begin{align*}
		H\left\{ \tilde{x}(t) \right\} & = \sin(t) \tilde{x}(t) \\
                                               & = \sin t x(t - t_0)    \\
                                               & \neq \sin(t - t_0) x(t - t_0)
	\end{align*}
	Therefore,
	\begin{align*}
		H\left\{ x(t - t_0) \right\} & \neq y(t - t_0)
	\end{align*}
	Therefore, the function is time dependent.
\end{solution}

\subsection{Linearity}

\begin{definition}
	Let
	\begin{align*}
		y_1(t) & = H\left\{ x_1(t) \right\} \\
		y_2(t) & = H\left\{ x_2(t) \right\}
	\end{align*}
	Then, the system is said to be linear, if and only if
	\begin{align*}
		H\left\{ a_1 x_1 + a_2 x_2 \right\} & = a_1 y_1 + a_2 y_2 \\
		H\{0\}                              & = 0
	\end{align*}
\end{definition}

\begin{question}
	Consider a system
	\begin{align*}
		y(t) &= T\left\{ x(t) \right\}\\
		&= \int\limits_{-\infty}^{t} x(\tau) \dif \tau
	\end{align*}
	Check for the following properties of the system.
	\begin{enumerate}
		\item Linearity
		\item Time invariance
		\item Causality
		\item Memory
		\item BIBO Stability
		\item Invertibility
	\end{enumerate}<++>
\end{question}<++>

\begin{solution}
	\begin{enumerate}[leftmargin=*]
		\item
			\begin{align*}
				T\left\{ a x_1(t) + b x_2(t) \right\} &= \int\limits_{-\infty}^{t} \left( a x_1(\tau) + b x_2(\tau) \right) \dif \tau\\
				&= a \int\limits_{-\infty}^{t} x_1(\tau) \dif \tau + b \int\limits_{-\infty}^{t} x_2(\tau) \dif \tau\\
				&= a T\left\{ x_1(t) \right\} + b T\left\{ x_2(t) \right\}
			\end{align*}
			Therefore, the system is linear.
		\item
			\begin{align*}
				T\left\{ x(t - T) \right\} &= \int\limits_{-\infty}^{t - T} x(\tau) \dif \tau\\
				&= \int\limits_{-\infty}^{t} x(\tau - T) \dif \tau\\
				&= y(t - T)
			\end{align*}
			Therefore, the system is time invariant.
		\item
			As $y(t)$ is dependent on the time from $-\infty$ to $t$, the system is causal.
		\item
			As $T$ is an integral from $-\infty$ to $t$, $y(t)$ depends on the historical inputs.
			Therefore, the system is not memoryless.
		\item
			Let
			\begin{align*}
				x(t) &= 1
			\end{align*}
			Therefore,
			\begin{align*}
				y(t) &= \int\limits_{-\infty}^{t} \dif \tau\\
				\therefore \lim\limits_{t \to \infty} y(t) &= \lim\limits_{t \to \infty} \int\limits_{-\infty}^{t} \dif \tau\\
				&\to \infty
			\end{align*}
			Therefore, as the system is unstable for a bounded input, it is BIBO unstable.
		\item
			\begin{align*}
				\dod{y(t)}{t} &= \dod{}{t} \int\limits_{-\infty}^{t} x(\tau) \dif \tau\\
				&= x(t)
			\end{align*}
			Therefore, the system is invertible.
	\end{enumerate}
\end{solution}

\section{LTI Systems}

\begin{definition}[Kronecker delta function]
	\begin{align*}
		\delta[n - n_0] &=
			\begin{cases}
				1 & ;\quad n = n_0    \\
				0 & ;\quad n \neq n_0 \\
			\end{cases}
	\end{align*}
	is called the Kronecker delta function.
\end{definition}

\begin{theorem}
	\begin{align*}
		x[n] \delta[n - n_0] & = x[n_0]
	\end{align*}
\end{theorem}

\begin{theorem}
	\begin{align*}
		x[n] & = \sum\limits_{k = -\infty}^{\infty} x[k] \delta[n - k]
	\end{align*}
\end{theorem}

\begin{definition}[Impulse response]
	The response of a system to an input $\delta[n - k]$ is called the impulse response of the system.
	It is denoted as $h_k[n]$.
\end{definition}

\begin{definition}[Discrete time convolution]
	\begin{align*}
		x[n] \ast h[n] & = \sum\limits_{k = -\infty}^{\infty} x[k] h[n - k] \\
                               & = \sum\limits_{k = -\infty}^{\infty} x[n - k] h[k]
	\end{align*}
\end{definition}

\begin{definition}[Continuous time convolution]
	\begin{align*}
		x(t) \ast h(t) & = \int\limits_{-\infty}^{\infty} x(\tau) h(t - \tau) \dif \tau \\
                               & = \int\limits_{-\infty}^{\infty} x(t - \tau) h(\tau) \dif \tau
	\end{align*}
\end{definition}

\begin{theorem}
	Consider a linear time invariant system such that
	\begin{align*}
		y[n] & = H\left\{ x[n] \right\}
	\end{align*}
	Then,
	\begin{align*}
		y[n] & = x[n] \ast h[n]
	\end{align*}
\end{theorem}

\begin{proof}
	\begin{align*}
		y[n] & = H\left\{ x[n] \right\} \\
                     & = H\left\{ \sum\limits_{k = -\infty}^{\infty} x[k] \delta[n - k] \right\}
	\end{align*}
	Therefore, as the system is linear,
	\begin{align*}
		y[n] & = \sum\limits_{k = -\infty}^{\infty} x[k] H\left\{ \delta[n - k] \right\} \\
                     & = \sum\limits_{k = -\infty}^{ \infty} x[k] h_k[n]
	\end{align*}
	Therefore, as the system is time invariant,
	\begin{align*}
		y[n] & = \sum\limits_{k = -\infty}^{\infty} x[k] h[n - k] \\
                     & = x[n] \ast h[n]
	\end{align*}
\end{proof}

\begin{theorem}
	Consider a linear time invariant system such that
	\begin{align*}
		y(t) & = H\left\{ x(t) \right\}
	\end{align*}
	Then,
	\begin{align*}
		y(t) & = x(t) \ast h(t)
	\end{align*}
\end{theorem}

\section{Transfer Functions}

\begin{definition}[Eigenvalues and eigenfunctions]
	A signal is said to be an eigenfunction of a system, if the output of the system for the signal is equal to the signal multiplied by a constant.
	The constant is called the eigenvalue corresponding to the eigenfunction.
\end{definition}

\subsection{Continuous Time Systems}

Let the impulse response of a system be $h(t)$.
Therefore, the response of the system for an input $e^{j \Omega t}$ is
\begin{align*}
	y(t) &= x(t) \ast h(t)\\
	&= \int\limits_{-\infty}^{\infty} h(\tau) x(t - \tau) \dif \tau\\
	&= \int\limits_{-\infty}^{\infty} h(\tau) e^{j \Omega (t - \tau)} \dif \tau\\
	&= \int\limits_{-\infty}^{\infty} h(\tau) e^{j \Omega t} e^{-j \Omega \tau} \dif \tau\\
	&= e^{j \Omega t} \int\limits_{-\infty}^{\infty} h(\tau) e^{-j \Omega \tau} \dif \tau
\end{align*}
Let
\begin{align*}
	H(s) &= \int\limits_{-\infty}^{\infty} h(\tau) e^{-s \tau} \dif \tau
\end{align*}
Therefore,
\begin{align*}
	y(t) &= H(j \Omega) e^{j \Omega t}
\end{align*}
$H$ is written as $H(j \Omega)$, to indicate that it is the transfer function $H(s)$, where $s = \sigma + j \Omega$, on the imaginary axis.

\subsection{Discrete Time Systems}

Let the impulse response of a system be $h[n]$.
Therefore, the response of the system for an input $e^{j \omega t}$ is
\begin{align*}
	y[n] &= x[n] \ast h[n]\\
	&= \sum\limits_{k = -\infty}^{\infty} h[k] x[n - k]\\
	&= \sum\limits_{k = -\infty}^{\infty} h[k] e^{j \omega (n - k)}\\
	&= \sum\limits_{k = -\infty}^{\infty} h[k] e^{j \omega k} e^{j \omega n}
\end{align*}
Let
\begin{align*}
	H\left( e^{j \omega} \right) &= \sum\limits_{k = -\infty}^{\infty} h[k] e^{-j \omega k}
\end{align*}
Therefore,
\begin{align*}
	y[n] &= H\left( e^{j \omega} \right) e^{j \omega n}
\end{align*}
$H$ is written as $H\left( e^{j \omega} \right)$, to indicate that it is the transfer function $H(z)$, where $z = \rho e^{j \omega}$, on the unit circle.

\section{Conjugate Symmetry for Real Impulse Response}

\begin{theorem}
	For a real valued $h(t)$,
	\begin{align*}
		H^*(j \Omega) &= H(-j \Omega)
	\end{align*}
\end{theorem}

\begin{proof}
	\begin{align*}
		H(j \Omega) &= \int\limits_{-\infty}^{\infty} h(t) e^{-j \Omega t} \dif t\\
		\therefore H^*(j \Omega) &= \int\limits_{-\infty}^{\infty} h(t) e^{j \Omega t} \dif t\\
		&= H(-j \Omega)
	\end{align*}
\end{proof}

\begin{theorem}
	For a real valued $h(t)$,
	\begin{align*}
		\Re\left\{ H(j \Omega) \right\} &= \Re\left\{ H(-j \Omega) \right\}\\
		\Im\left\{ H(j \Omega) \right\} &= -\Im\left\{ H(-j \Omega) \right\}
	\end{align*}
\end{theorem}

\begin{theorem}
	For a real valued $h(t)$,
	\begin{align*}
		\left| H(j \Omega) \right| &= \left| H(-j \Omega) \right|\\
		\angle H(j \Omega) &= -\angle H(-j \Omega)
	\end{align*}
\end{theorem}

\section{Response of a System with Real Valued Impulse Response to a Real Valued Harmonic Function}

Let
\begin{align*}
	x(t) &= a \cos(\Omega t)\\
	&= \frac{a}{2} e^{j \Omega t} + \frac{a}{2} e^{-j \Omega t}
\end{align*}
As the impulse response is real,
\begin{align*}
	h(t) &= h^*(t)
\end{align*}
Therefore,
\begin{align*}
	y(t) &= \frac{a}{2} H(j \Omega) e^{j \Omega t} + \frac{a}{2} H(-j \Omega) e^{-j \Omega t}\\
	&= \frac{a}{2} H(j \Omega) e^{j \Omega t} + \frac{a}{2} H^*(j \Omega) e^{-j \Omega t}
\end{align*}
Let
\begin{align*}
	z &= H(j \Omega) e^{j \Omega t}\\
	\therefore z^* &= H^*(j \Omega) e^{-j \Omega t}\\
\end{align*}
Therefore,
\begin{align*}
	y(t) &= \frac{a}{2} z + \frac{a}{2} z^*\\
	&= a \Re\{z\}\\
	&= a \Re\left\{ H(j \Omega) e^{j \Omega t} \right\}\\
	&= a \Re\left\{ \left| H(j \Omega) \right| e^{j \angle H(j \Omega)} e^{j \Omega t} \right\}\\
	&= a \left| H(j \Omega) \right| \Re\left\{ e^{j \left( \Omega t + \angle H(j \Omega) \right)} \right\}\\
	&= a \left| H(j \Omega) \right| \cos\left( \Omega t + \angle H(j \Omega) \right)
\end{align*}

\section{Power and Energy}

\begin{definition}[Power and energy for continuous time systems]
	The power and energy for a finite interval are defined as
	\begin{align*}
		E_x &= \int\limits_{t_1}^{t_2} \left| x(t) \right|^2 \dif t\\
		\langle P \rangle &= \frac{1}{t_2 - t_1} \int\limits_{t_1}^{t_2} \left| x(t) \right|^2 \dif t
	\end{align*}
	The power and energy for an infinite interval are defined as
	\begin{align*}
		E_{\infty} &= \lim\limits_{T \to \infty} \int\limits_{-T}^{T} \left| x(t) \right|^2 \dif t\\
		\langle P \rangle &= \lim\limits_{T \to \infty} \frac{1}{2 T} \int\limits_{-T}^{T} \left| x(t) \right|^2 \dif t
	\end{align*}
\end{definition}

\begin{definition}[Power and energy for discrete time systems]
	The power and energy for a finite interval are defined as
	\begin{align*}
		E_x &= \sum\limits_{n = n_1}^{n^2} \left| x[n] \right|^2\\
		\langle P \rangle &= \frac{1}{n_2 - n_1} \sum\limits_{n = n_1}^{n_2} \left| x[n] \right|^2
	\end{align*}
	The power and energy for an infinite interval are defined as
	\begin{align*}
		E_{\infty} &= \lim\limits_{T \to \infty} \sum\limits_{n = -N}^{N} \left| x[n] \right|^2\\
		\langle P \rangle &= \lim\limits_{T \to \infty} \frac{1}{2 N + 1} \left| x[n] \right|^2
	\end{align*}
\end{definition}

\begin{definition}[Energy signal]
	A signal $x$ is said to be a energy signal if $E_{\infty}$ is finite.
\end{definition}

\begin{definition}[Power signal]
	A signal $x$ is said to be a power signal if $P_{\infty}$ is finite and positive.
\end{definition}

\clearpage
\part{Fourier Series}

\section{Synthesis and Analysis}

\subsection{Discrete Time Systems}

Let
\begin{align*}
	\varphi_k[n] &= e^{j k \frac{2 \pi}{N} n}
\end{align*}
where $k = 0,\dots,N - 1$.\\
Therefore,
\begin{align*}
	\left\langle \varphi_k[n],\varphi_m[n] \right\rangle &= \sum\limits_{n = \langle N \rangle} \varphi_k[n] {\varphi_m}^*[n]\\
	&= \sum\limits_{n = \langle N \rangle} e^{j k \frac{2 \pi}{N} n} e^{-j m \frac{2 \pi}{N} n}\\
	&= \sum\limits_{n = \langle N \rangle} e^{j (k - m) \frac{2 \pi}{N} n}
\end{align*}
If $k = m + l N$, where $l \in \mathbb{Z}$,
\begin{align*}
	\left\langle \varphi_k[n],\varphi_m[n] \right\rangle &= \sum\limits_{n = \langle N \rangle} 1\\
	&= N
\end{align*}
If $k \neq m + l N$, where $l \in \mathbb{Z}$, let
\begin{align*}
	W_n &= e^{j \frac{2 \pi}{N}}
\end{align*}
Therefore,
\begin{align*}
	\left\langle \varphi_k[n],\varphi_m[n] \right\rangle &= \sum\limits_{n = \langle N \rangle} {W_N}^{(k - m) n}
\end{align*}
Let
\begin{align*}
	p &= k - m
\end{align*}
Therefore, $p \in \mathbb{Z} \setminus \{0,\pm N,\pm 2 N,\dots\}$.
Therefore,
\begin{align*}
	\left\langle \varphi_k[n],\varphi_m[n] \right\rangle &= \sum\limits_{n = \langle N \rangle} {W_N}^{p n}\\
	&= \sum\limits_{n = 0}^{N - 1} {W_N}^{p n}\\
	&= \frac{1 - {W_N}^{p N}}{1 - {W_N}^p}\\
	&= \frac{1 - e^{j \frac{2 \pi}{N} p N}}{1 - e^{j \frac{2 \pi}{N} p}}\\
	&= 0
\end{align*}
Therefore,
\begin{align*}
	\left\langle \varphi_k[n],\varphi_m[n] \right\rangle &=
		\begin{cases}
			N &;\quad k = m + l N, l \in \mathbb{Z}\\
			0 &;\quad \text{otherwise}\\
		\end{cases}\\
		&= N \delta[k - m - l N]
\end{align*}
Let
\begin{align*}
	x[n] &= \sum\limits_{k = \langle N \rangle} a_k \varphi_k[n]
\end{align*}
Therefore,
\begin{align*}
	\left\langle x[n],\varphi_k[n] \right\rangle &= \left\langle \sum\limits_{m = \langle N \rangle} a_m \varphi_m[n] , \varphi_k[n] \right\rangle\\
	&= \sum\limits_{m = \langle N \rangle} \left\langle \varphi_m[n],\varphi_k[n] \right\rangle\\
	&= \sum\limits_{m = \langle N \rangle} a_m N \delta[m - k]\\
	&= N a_k
\end{align*}
Therefore,
\begin{align*}
	a_k &= \frac{1}{N} \left\langle x[n],\varphi_k[n] \right\rangle\\
	&= \frac{1}{N} \sum\limits_{n = \langle N \rangle} x[n] {\varphi_k}^*[n]\\
	&= \frac{1}{N} \sum\limits_{n = \langle N \rangle} x[n] e^{-j k \frac{2 \pi}{N} n}
\end{align*}
Therefore,
\begin{align*}
	x[n] &= \sum\limits_{k = \langle N \rangle} a_k e^{j k \frac{2 \pi}{N} n}
\end{align*}

\begin{theorem}[Analysis formula for discrete time systems]
	\begin{align*}
		a_k &= \frac{1}{N} \sum\limits_{n = \langle N \rangle} x[n] e^{-j k \frac{2 \pi}{N} n}\\
	\end{align*}
	\label{thm:Analysis_formula_for_discrete_time_systems}
\end{theorem}

\begin{theorem}[Synthesis formula for discrete time systems]
	\begin{align*}
		x[n] &= \sum\limits_{k = \langle N \rangle} a_k e^{j k \frac{2 \pi}{N} n}
	\end{align*}
	\label{thm:Synthesis_formula_for_discrete_time_systems}
\end{theorem}

\begin{question}
	Find the coefficients $a_k$ for
	\begin{align*}
		x[n] &= \sin(\omega_0 n)
	\end{align*}
\end{question}

\begin{solution}
	\begin{align*}
		\frac{\omega_0}{2 \pi} &= \frac{m}{N}
	\end{align*}
	Therefore, the function is
	\begin{align*}
		x[n] &= \sin(\omega_0 n)\\
		&= \frac{1}{2 j} e^{j \frac{2 \pi}{N} n} - \frac{1}{2 j} e^{-j \frac{2 \pi}{N} n}
	\end{align*}
	Let
	\begin{align*}
		\omega_0 &= \frac{2 \pi}{N}
	\end{align*}
	Therefore, the coefficients are
	\begin{align*}
		a_1 &= \frac{1}{2 j}\\
		a_{N - 1} &= a_{-1}\\
		&= -\frac{1}{2 j}
	\end{align*}
	All other coefficients are zero.
\end{solution}

\begin{question}
	Find the coefficients $a_k$ for
	\begin{align*}
		x[n] &= \sin\left( \frac{6 \pi}{8} n \right)
	\end{align*}
\end{question}

\begin{solution}
	\begin{align*}
		\frac{6 \pi}{8} &= 3 \cdot \frac{2 \pi}{8}
	\end{align*}
	Let
	\begin{align*}
		\omega_0 &= \frac{2 \pi}{8}
	\end{align*}
	Therefore,
	\begin{align*}
		x[n] &= \sin\left( \frac{6 \pi}{8} n \right)\\
		&= \frac{1}{2 j} e^{j \frac{6 \pi}{8} n} - \frac{1}{2 j} e^{-j \frac{6 \pi}{8} n}
	\end{align*}
	Therefore, the coefficients are
	\begin{align*}
		a_3 &= \frac{1}{2 j}\\
		a_5 &= a_{-3}\\
		&= -\frac{1}{2 j}
	\end{align*}
\end{solution}

\subsection{Continuous Time Systems}

\begin{align*}
	x(t) &= x(t + T)\\
	\Omega_0 &= \frac{2 \pi}{T}
\end{align*}
Let
\begin{align*}
	\varphi_k(t) &= e^{j k \Omega_0 t}
\end{align*}
where $k \in \mathbb{Z}$.\\
Therefore,
\begin{align*}
	\left\langle \varphi_k(t),\varphi_m(t) \right\rangle &= \int\limits_{T} \varphi_k(t) {\varphi_m}^*(t) \dif t\\
	&= \int\limits_{T} \left( e^{j k \Omega_0 t} e^{-j m \Omega_0 t} \right) \dif t\\
	&= \int\limits_{T} e^{j (k - m) \Omega_0 t} \dif t\\
	&=
		\begin{cases}
			T &;\quad k = m\\
			0 &;\quad k \neq m\\
		\end{cases}\\
	&= T \delta[k - m]
\end{align*}
Therefore,
\begin{align*}
	x(t) &= \sum\limits_{k = -\infty}^{\infty} a_k \varphi_k(t)\\
	&= \sum\limits_{k = -\infty}^{\infty} a_k e^{j k \Omega_0 t}
\end{align*}
Therefore,
\begin{align*}
	\left\langle x(t),\varphi_k(t) \right\rangle &= \left\langle \sum\limits_{m = -\infty}^{\infty} a_m \varphi_m(t) , \varphi_k(t) \right\rangle\\
	&= \sum\limits_{m = -\infty}^{\infty} a_m \left\langle \varphi_m(t),\varphi_k(t) \right\rangle\\
	&= \sum\limits_{m = -\infty}^{\infty} a_m T \delta[k - m]\\
	&= T a_k
\end{align*}
Therefore,
\begin{align*}
	a_k &= \frac{1}{T} \left\langle x(t),\varphi_k(t) \right\rangle\\
	&= \frac{1}{T} \int\limits_{T} x(t) e^{-j k \Omega_0 t} \dif t\\
\end{align*}

\begin{theorem}[Analysis formula for continuous time systems]
	\begin{align*}
		a_k &= \frac{1}{T} \int\limits_{T} x(t) e^{-j k \Omega_0 t}
	\end{align*}
	\label{thm:Analysis_formula_for_continuous_time_systems}
\end{theorem}

\begin{theorem}[Synthesis formula for continuous time systems]
	\begin{align*}
		x(t) &= \sum\limits_{k = -\infty}^{\infty} a_k e^{j k \Omega_0 t}
	\end{align*}
	\label{thm:Synthesis_formula_for_continuous_time_systems}
\end{theorem}

\section{Periodic Square Waves}

\begin{definition}[Normalized sinus cardinalis/$\sinc$ function]
	\begin{align*}
		\sinc(x) &=
			\begin{cases}
				1 &;\quad x = 0\\
				\frac{\sin(\pi x)}{\pi x} &;\quad x\neq 0\\
			\end{cases}
	\end{align*}
\end{definition}

\begin{theorem}
	\begin{align*}
		\int\limits_{-\infty}^{\infty} \sinc(x) \dif x &= 1
	\end{align*}
\end{theorem}

\begin{theorem}
	\begin{align*}
		\sinc(x) &< \frac{1}{|x|}
	\end{align*}
\end{theorem}

\subsection{Continuous Time Signal}

Consider a square wave, such that
\begin{align*}
	x(t) &= x(t + T)\\
	x(t) &=
		\begin{cases}
			1 &;\quad |t| \le T_1\\
			0 &;\quad T_1 < |t| \le \frac{T}{2}\\
		\end{cases}
\end{align*}
Let
\begin{align*}
	\Omega_0 &= \frac{2 \pi}{T}
\end{align*}
Therefore, by \nameref{thm:Analysis_formula_for_continuous_time_systems}, the Fourier coefficients are
\begin{align*}
	a_0 &= \frac{1}{T} \int\limits_{T} x(t) \dif t\\
	&= \frac{1}{T} \int\limits_{-T_1}^{T_1} \dif t\\
	&= \frac{2 T_1}{T}
\end{align*}
For $k \neq 0$,
\begin{align*}
	a_k &= \frac{1}{T} \int\limits_{T} x(t) e^{-j k \Omega_0 t} \dif t\\
	&= \frac{1}{T} \left. \frac{e^{-j k \Omega_0 t}}{-j k \Omega_0} \right|_{-T_1}^{T_1}\\
	&= \frac{e^{j k \Omega_0 T_1} - e^{-j k \Omega_0 T_1}}{j k \Omega_0 T}\\
	&= \frac{2}{k \Omega_0 T} \frac{e^{j k \Omega_0 T_1} - e^{-j k \Omega_0 T_1}}{2 j}\\
	&= \frac{2 \sin(k \Omega_0 T_1)}{k \Omega_0 T}\\
	&= \frac{\sin(k \Omega_0 T_1)}{k \pi}\\
	&= \frac{2 \sin(k \Omega_0 T_1)}{k \Omega_0 T}\\
	&= \frac{2 T_1}{T} \frac{\sin(k \Omega_0 T_1)}{k \Omega_0 T_1}
\end{align*}
Therefore, $\forall k \in \mathbb{Z}$,
\begin{align*}
	a_k &= \frac{2 T_1}{T} \sinc\left( \frac{k \Omega_0 T_1}{\pi} \right)\\
	&= \frac{2 T_1}{T} \sinc\left( k \frac{2 T_1}{T} \right)
\end{align*}

\subsection{Discrete Time Signal}

Consider a square wave, such that
\begin{align*}
	x[n] &= x[n + N]\\
	x[n] &=
		\begin{cases}
			1 &;\quad |n| \le N_1\\
			0 &;\quad N_1 < |n| \le \frac{N}{2}\\
		\end{cases}
\end{align*}
Let
\begin{align*}
	\omega_0 &= \frac{2 \pi}{N}
\end{align*}
Therefore, by \nameref{thm:Analysis_formula_for_discrete_time_systems}, the Fourier coefficients are
\begin{align*}
	a_k &= \frac{1}{N} \sum\limits_{n = \langle N \rangle} x[n] e^{-j k \frac{2 \pi}{N} n}\\
	&= \frac{1}{N} \sum\limits_{n = -N_1}^{N_1} e^{-j k \frac{2 \pi}{N} n}\\
	&= \frac{1}{N} \sum\limits_{l = 0}^{2 N_1} e^{-j k \frac{2 \pi}{N} (l - N_1)}\\
	&= \frac{e^{j k \frac{2 \pi}{N} N_1}}{N} \sum\limits_{l = 0}^{2 N_1} e^{-j k \frac{2 \pi}{N} l}\\
	&= \frac{e^{j k \frac{2 \pi}{N} N_1}}{N} \frac{1 - e^{-j k \frac{2 \pi}{N} (2 N_1 + 1)}}{1 - e^{-j k \frac{2 \pi}{N}}}\\
	&= \frac{e^{j k \frac{2 \pi}{N} N_1}}{N} \frac{e^{-j k \frac{2 \pi}{N} \frac{2 N_1 + 1}{2}}}{e^{-j k \frac{2 \pi}{2 N}}} \frac{e^{j k \frac{2 \pi}{N} \frac{2 N_1 + 1}{2}} - e^{-j k \frac{2 \pi}{N} \frac{2 N_1 + 1}{2}}}{e^{j k \frac{2 \pi}{2 N}} - e^{-j k \frac{2 \pi}{2 N}}}\\
	&= \frac{1}{N} \frac{\sin\left( k \frac{2 \pi (2 N_1 + 1)}{2 N} \right)}{\sin\left( k \frac{2 \pi}{2 N} \right)}\\
	&= \frac{1}{N} \frac{\sin\left( \frac{k \pi}{N} (2 N_1 + 1) \right)}{\sin\left( \frac{k \pi}{N} \right)}
\end{align*}

\section{Convergence of Fourier Series and Projection Theorem}

\begin{theorem}[Best Approximation Theorem]
	Let $\left\{ \varphi_k(t) \right\}$ be a set of orthonormal function over $[a,b)$, where $k \in \mathbb{W}$.\\
	Let $x(t)$ be defined on $[a,b)$.\\
	Let
	\begin{align*}
		\hat{x}_N(t) &= \sum\limits_{k = -N}^{N} a_k \varphi_k(t)
	\end{align*}
	be an approximation of $x(t)$.\\
	Let
	\begin{align*}
		e_N(t) &= x(t) - \hat{x}_N(t)
	\end{align*}
	be the error in the approximation.\\
	Let the energy corresponding to the error be
	\begin{align*}
		E_N &= \int\limits_{a}^{b} \left| e_N(t) \right|^2 \dif t
	\end{align*}
	Then, $E_N$ is minimized by the selection
	\begin{align*}
		a_k &= \left\langle x(t),{\varphi_k}(t) \right\rangle\\
		&= \int\limits_{a}^{b} x(t) {\varphi_k}^*(t) \dif t
	\end{align*}
	Also, if $x \in L_2$,
	\begin{align*}
		\lim\limits_{N \to \infty} E_N &= 0
	\end{align*}
	\label{thm:Best_Approximation_Theorem}
\end{theorem}

\begin{theorem}[Dirichlet's Conditions]
	If a signal $x(t)$ satisfies
	\begin{enumerate}
		\item $\displaystyle \int\limits_{T} \left| x(t) \right| \dif t < \infty$.
		\item In every finite interval, $x(t)$ has a finite number of extrema.
		\item In every finite interval, $x(t)$ has a finite number of discontinuities, each of which is finite.
	\end{enumerate}
	then,
	\begin{align*}
		\lim\limits_{N \to \infty} x_N(t) &= \frac{x(t^+) + x(t^-)}{2}
	\end{align*}
	\label{thm:Dirichlet's_Conditions}
\end{theorem}

\begin{theorem}
	If $x(t)$ is periodic, with discontinuities, then, $\forall k > K$,
	\begin{align*}
		|a_k| &< \frac{c}{k}
	\end{align*}
	If the first $m$ derivatives of $x(t)$ are continuous,
	\begin{align*}
		|a_k| &< \frac{c}{k^{m + 2}}
	\end{align*}
\end{theorem}

\begin{theorem}[Gibbs' Phenomenon]
	For every point of continuity,
	\begin{align*}
		\lim\limits_{N \to \infty} x_N &= x(t)
	\end{align*}
	At every point of discontinuity,
	\begin{align*}
		\lim\limits_{N \to \infty} \left| x(t + \Delta t) - x_N(t + \Delta t) \right| &\approx 0.09 \Delta x
	\end{align*}
	where
	\begin{align*}
		\Delta t &= \frac{T}{2 N}
	\end{align*}
	\label{thm:Gibbs'_Phenomenon}
\end{theorem}

\section{Properties of Fourier Series}

\subsection{Time Reversal}

Consider a system
\begin{align*}
	y(t) &= x(-t)
\end{align*}
Let $x(t) \FS a_k$, i.e.,
\begin{align*}
	x(t) &= \sum\limits_{k = -\infty}^{\infty} a_k e^{j k \Omega_0 t}
\end{align*}
Therefore,
\begin{align*}
	y(t) &= x(-t)\\
	&= \sum\limits_{k = -\infty}^{\infty} a_k e^{j k \Omega_0 (-t)}
\end{align*}
Let
\begin{align*}
	m &= -k
\end{align*}
Therefore,
\begin{align*}
	y(t) &= \sum\limits_{m = -\infty}^{\infty} a_{-m} e^{j m \Omega_0 t}
\end{align*}
Let
\begin{align*}
	b_m &= a_{-m}
\end{align*}
Therefore,
\begin{align*}
	y(t) &= \sum\limits_{m = -\infty}^{\infty} b_m e^{j m \Omega_0 t}
\end{align*}
Therefore, in general, if
\begin{align*}
	x(t) &\FS a_k
\end{align*}
then
\begin{align*}
	x(-t) &\FS a_{-k}
\end{align*}
Similarly, in general, if
\begin{align*}
	x[n] &\DFS a_k
\end{align*}
then
\begin{align*}
	x[-n] &\DFS a_{-k}
\end{align*}

\subsection{Scaling}

Consider a system
\begin{align*}
	y(t) &= x(\alpha t)
\end{align*}
Let
\begin{align*}
	\tilde{\Omega}_0 &= \frac{2 \pi}{\tilde{T}}\\
	&= \alpha \frac{2 \pi}{T}\\
	&= \alpha \Omega_0
\end{align*}
Let
\begin{align*}
	\alpha \tilde{T} &= T
\end{align*}
Therefore,
\begin{align*}
	y(t) &= x(\alpha t)\\
	&= \sum\limits_{k = -\infty}^{\infty} a_k e^{j k \frac{\tilde{\Omega}}{\tilde{\Omega}_0 \alpha} t}\\
	&= \sum\limits_{k = -\infty}^{\infty} a_k e^{j k \tilde{\Omega}_0 t}
\end{align*}

\end{document}
